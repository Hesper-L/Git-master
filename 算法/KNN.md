# KNN

> 最近邻（K-Nearest Neighbor）
>
> 有监督算法、分类算法，也能用于回归。

KNN（K-Nearest Neighbor）算法是机器学习算法中最基础、最简单的算法之一。KNN通过**测量不同特征值之间的距离来进行分类**。

#### mindset

对于任意n维输入向量，分别对应于特征空间中的一个点，输出为该特征向量所对应的类别标签或预测值。

#### when

- 不可能所有测试对象都有完全匹配的训练对象
- 存在一个测试对象同时与多个训练对象匹配
- 产生KNN算法
- 如果一个样本在特征空间中的k个最相似（即特征空间中最邻近）的样本的大多数属于同个类别，则该样本也属于这个类别
- $k\le20(k\in N)$，算法结果很大程度取决于K的选择

####   优势

1. 通过计算对象间的距离来作为各个对象之间的非相似性指标，避免对象之间的匹配问题
2. 距离一般采用**欧式距离**（距离之差的平方和再开平方根）或曼哈顿距离（距离绝对值之差的和再开平方根）
3. 依据k个对象中占优的类别进行决策，而不是单一的对象类别决策

####   算法描述

1. 计算测试数据与各个训练数据之间的距离；
2. 按照距离的递增关系进行排序；
3. 选取距离最小的K个点；
4. 确定前K个点所在类别的出现频率；
5. 返回前K个点中出现频率最高的类别作为测试数据的预测分类。   

####   Advantage

1. 理论成熟，思想简单，既可以用来做分类也可以用来做回归　　　　
2. 可用于非线性分类
3. 训练时间复杂度比支持向量机之类的算法低，仅为O(n)
4. 和朴素贝叶斯之类的算法比，对数据没有假设，准确度高，对异常点不敏感
5. 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合
6. 该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分

####   Disadvantage

1. 计算量大，分类速度慢
	改进：浓缩训练样本集 ；加快K个最近邻的搜索速度
2. 属性较多时降低效率。
	KNN在对属性较多的训练样本进行分类时，由于计算量大而使其效率大大降低效果。懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢
3. K值难以确定 
	目前没有很好的方法，一般采用先定一个初始值，然后根据实验测试的结果调整K值。 
4.  对不平衡样本集比较敏感
	当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。改进：采用权值的方法（增大距离小的邻居样本的权值）



```python
import numpy as np


Dataset = np.array([[1.0, 2.0], [1.2, 0.1], [0.1, 1.4], [0.3, 3.5]])

labels = ['A', 'A', 'B', 'B']

test = [1.1, 0.3]

k = 3

#分类

diff = test - Dataset

squaredist = np.sum(diff**2, axis=1)

# 当axis为0时,是压缩行,即将每一列的元素相加,将矩阵压缩为一行

# 当axis为1时,是压缩列,即将每一行的元素相加,将矩阵压缩为一列

dist = squaredist**0.5

index = np.argsort(dist)  #np.argsort(-x) 按降序排列

dict = {}

for i in range(k):

  vote = labels[index[i]]

  dict[vote] = dict.get(

   vote, 0) + 1  #dict.get(key, default=None)返回指定键的值，如果值不在字典中返回默认值None。

max = 0

for key, value in dict.items(): 
    #以列表返回可遍历的(键, 值) 元组数组
  if max < value:

     max = value

     classes = key


print(classes)
```

